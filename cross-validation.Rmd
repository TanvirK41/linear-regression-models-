---
title: "cross_validation"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)
```

Cross Validation is very general tool, you can compare all kinds of different things 


# Understanding of how you can use prediction accuracy to compare models for varying degree of complexity 


## Simulate Data
```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - .3) ^ 2 + rnorm(100, 0, .3)
  )
```


## Look at the data
```{r}
nonlin_df %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

## Cross Validation -- by hand (construct a training set and testing set)
```{r}
# Generate training and testing data set
set.seed(1)
train_df = sample_n(nonlin_df, size = 80) %>% 
  arrange("id")

test_df = anti_join(nonlin_df, train_df, by = "id")
```


## Fit three models 
```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = gam(y ~ s(x), data = train_df)
wiggly_mod = gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df) 
```


## can i see what I just did ......
```{r}
train_df %>%
  add_predictions(linear_mod) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```


```{r}
train_df %>%
  add_predictions(smooth_mod) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```


```{r}
train_df %>%
  add_predictions(wiggly_mod) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = pred), color = "red")
```


```{r}
train_df %>%
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(. ~ model)
```



## Look at prediction accuracy - can we compute the root mean square for the model looking not at the root mean sqaure mean for this dataset but how well its going about in making prediction on an unobserved data set. Root Mean Square in testing dataset
```{r}
# missing patterns 
rmse(linear_mod, test_df)
# best choice - best predictions 
rmse(smooth_mod, test_df)
# chasing things that are noise and not real things 
rmse(wiggly_mod, test_df)
```




## Cross Validation using 'modelr'

```{r}
cv_df = 
  crossv_mc(nonlin_df, n = 100)
```


## What is happening here?

```{r}
cv_df %>%
  pull(train) %>% .[[1]] %>%
  as_tibble()

cv_df %>%
  pull(test)  %>% .[[1]] %>%
  as_tibble()
```

**resample objects can be put into lm(), but can not be put into gam()** 
```{r}
cv_df = 
  cv_df %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```



## Let's try to fit models and get RMSEs for them. 
```{r}
cv_df = 
  cv_df %>%
  mutate(
    linear_mod = map(.x = train, ~lm(y ~ x, data = .x)), 
    smooth_mod = map(.x = train, ~gam(y ~ s(x), data = .x)),
    wiggly_mod = map(train, ~gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))) %>%
  mutate(
    rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(model = .x, data = .y)), 
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)),
    rmse_wiggly = map2_dbl(wiggly_mod, test, ~rmse(model = .x, data = .y)))
```


## What do these results say about the model choice 
```{r}
#distribution across 100 training, testing splits, fitting the linear, smooth, and wiggly model
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    #gets rid of rmse_ in the code line below
    names_prefix = "rmse_"
  ) %>%
  ggplot(aes(x = model, y  = rmse)) +
  geom_violin()
#linear models looks worse - not too complex 
#smooth model on average looks better 
#wiggly looks better than linear - complex 
```


## Compute Averages
```{r}
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    #gets rid of rmse_ in the code line below
    names_prefix = "rmse_"
  ) %>%
  group_by(model) %>%
  summarise(avg_rmse = mean(rmse))
```

